{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wetland Training Dataset Creator - 1.5M Samples\n",
        "\n",
        "**Output:** `wetland_dataset_1.5M_4Training.npz`\n",
        "\n",
        "**Features:**\n",
        "- ‚úÖ All 6 classes (0-5) including background\n",
        "- ‚úÖ Filters NaN values properly\n",
        "- ‚úÖ Balanced 1.5M samples\n",
        "- ‚úÖ Includes class weights\n",
        "- ‚úÖ Memory-efficient chunk processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 1: Setup\n",
        "print(\"üöÄ Setting up environment...\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"‚úì Drive already mounted\")\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q rasterio tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import rasterio\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: Configuration\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Paths\n",
        "labels_file = \"/content/drive/MyDrive/bow_river_wetlands_10m_final.tif\"\n",
        "embeddings_dir = Path(\"/content/drive/MyDrive/EarthEngine\")\n",
        "output_file = \"/content/drive/MyDrive/wetland_dataset_1.5M_4Training.npz\"\n",
        "\n",
        "# Balanced sampling - ALL 6 CLASSES\n",
        "samples_per_class = {\n",
        "    0: 600_000,   # Background - INCLUDED\n",
        "    1: 19_225,\n",
        "    2: 150_000,\n",
        "    3: 500_000,\n",
        "    4: 150_000,\n",
        "    5: 100_000,\n",
        "}\n",
        "\n",
        "print(f\"\\nLabels: {labels_file}\")\n",
        "print(f\"Embeddings: {embeddings_dir}\")\n",
        "print(f\"Output: {output_file}\")\n",
        "print(f\"\\nTarget: {sum(samples_per_class.values()):,} samples\")\n",
        "\n",
        "for cls, count in samples_per_class.items():\n",
        "    print(f\"  Class {cls}: {count:,}\")\n",
        "\n",
        "# Verify paths\n",
        "assert os.path.exists(labels_file), f\"‚ùå Labels not found: {labels_file}\"\n",
        "assert embeddings_dir.exists(), f\"‚ùå Embeddings not found: {embeddings_dir}\"\n",
        "\n",
        "tile_files = sorted(embeddings_dir.glob(\"*.tif\"))\n",
        "print(f\"\\n‚úì Found {len(tile_files)} embedding tiles\")\n",
        "print(\"‚úÖ Configuration validated!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: Sample Pixel Coordinates (Memory-Efficient)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLING COORDINATES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "sampled_coords = {cls: {'y': [], 'x': []} for cls in samples_per_class.keys()}\n",
        "samples_collected = {cls: 0 for cls in samples_per_class.keys()}\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nScanning labels in chunks...\")\n",
        "with rasterio.open(labels_file) as src:\n",
        "    windows = list(src.block_windows(1))\n",
        "    np.random.shuffle(windows)\n",
        "    \n",
        "    for idx, (block_id, window) in tqdm(enumerate(windows), total=len(windows), desc=\"Blocks\"):\n",
        "        labels_chunk = src.read(1, window=window)\n",
        "        row_off = window.row_off\n",
        "        col_off = window.col_off\n",
        "        \n",
        "        for cls in samples_per_class.keys():\n",
        "            if samples_collected[cls] >= samples_per_class[cls]:\n",
        "                continue\n",
        "            \n",
        "            class_mask = (labels_chunk == cls)\n",
        "            y_local, x_local = np.where(class_mask)\n",
        "            \n",
        "            if len(y_local) == 0:\n",
        "                continue\n",
        "            \n",
        "            y_global = y_local + row_off\n",
        "            x_global = x_local + col_off\n",
        "            \n",
        "            needed = samples_per_class[cls] - samples_collected[cls]\n",
        "            available = len(y_local)\n",
        "            n_sample = min(needed, available)\n",
        "            \n",
        "            if available > needed:\n",
        "                idx_sample = np.random.choice(available, n_sample, replace=False)\n",
        "                sampled_coords[cls]['y'].append(y_global[idx_sample])\n",
        "                sampled_coords[cls]['x'].append(x_global[idx_sample])\n",
        "            else:\n",
        "                sampled_coords[cls]['y'].append(y_global)\n",
        "                sampled_coords[cls]['x'].append(x_global)\n",
        "            \n",
        "            samples_collected[cls] += n_sample\n",
        "        \n",
        "        if all(samples_collected[cls] >= samples_per_class[cls] for cls in samples_per_class.keys()):\n",
        "            print(f\"\\n‚úì Got all samples after {idx+1}/{len(windows)} blocks\")\n",
        "            break\n",
        "\n",
        "# Combine\n",
        "all_y, all_x, all_labels = [], [], []\n",
        "\n",
        "for cls in samples_per_class.keys():\n",
        "    if len(sampled_coords[cls]['y']) > 0:\n",
        "        y_coords = np.concatenate(sampled_coords[cls]['y'])\n",
        "        x_coords = np.concatenate(sampled_coords[cls]['x'])\n",
        "        \n",
        "        if len(y_coords) > samples_per_class[cls]:\n",
        "            y_coords = y_coords[:samples_per_class[cls]]\n",
        "            x_coords = x_coords[:samples_per_class[cls]]\n",
        "        \n",
        "        all_y.append(y_coords)\n",
        "        all_x.append(x_coords)\n",
        "        all_labels.append(np.full(len(y_coords), cls))\n",
        "        print(f\"  Class {cls}: {len(y_coords):,}\")\n",
        "\n",
        "y_indices = np.concatenate(all_y)\n",
        "x_indices = np.concatenate(all_x)\n",
        "y = np.concatenate(all_labels)\n",
        "\n",
        "shuffle_idx = np.random.permutation(len(y_indices))\n",
        "y_indices = y_indices[shuffle_idx]\n",
        "x_indices = x_indices[shuffle_idx]\n",
        "y = y[shuffle_idx]\n",
        "\n",
        "print(f\"\\nTotal coordinates: {len(y):,}\")\n",
        "print(\"‚úÖ Coordinates sampled!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Extract Embeddings from Tiles\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXTRACTING EMBEDDINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "n_samples = len(y_indices)\n",
        "X = np.zeros((n_samples, 64), dtype=np.float32)\n",
        "found_samples = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "with tqdm(total=len(tile_files), desc=\"Tiles\", unit=\" tiles\") as pbar:\n",
        "    for tile_file in tile_files:\n",
        "        with rasterio.open(tile_file) as tile_src:\n",
        "            # Parse tile position from filename\n",
        "            parts = tile_file.stem.split('-')\n",
        "            if len(parts) >= 3:\n",
        "                try:\n",
        "                    tile_row_offset = int(parts[-2])\n",
        "                    tile_col_offset = int(parts[-1])\n",
        "                except ValueError:\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "            else:\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "            \n",
        "            tile_height, tile_width = tile_src.height, tile_src.width\n",
        "            \n",
        "            # Find samples in this tile\n",
        "            in_tile_y = (y_indices >= tile_row_offset) & (y_indices < tile_row_offset + tile_height)\n",
        "            in_tile_x = (x_indices >= tile_col_offset) & (x_indices < tile_col_offset + tile_width)\n",
        "            in_tile_mask = in_tile_y & in_tile_x\n",
        "            \n",
        "            if in_tile_mask.any():\n",
        "                tile_data = tile_src.read()  # (64, H, W)\n",
        "                \n",
        "                local_y = y_indices[in_tile_mask] - tile_row_offset\n",
        "                local_x = x_indices[in_tile_mask] - tile_col_offset\n",
        "                \n",
        "                for i, (ly, lx) in enumerate(zip(local_y, local_x)):\n",
        "                    global_idx = np.where(in_tile_mask)[0][i]\n",
        "                    pixel_values = tile_data[:, ly, lx]\n",
        "                    \n",
        "                    # Filter samples with ANY NaN (standard ML practice)\n",
        "                    # NOTE: If you need to keep some NaN, change this condition\n",
        "                    if not np.isnan(pixel_values).any():\n",
        "                        X[global_idx, :] = pixel_values\n",
        "                        found_samples[global_idx] = True\n",
        "        \n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({\"found\": f\"{found_samples.sum():,}/{n_samples:,}\"})\n",
        "\n",
        "print(f\"\\n‚úì Extracted {found_samples.sum():,} / {n_samples:,} samples\")\n",
        "\n",
        "if not found_samples.all():\n",
        "    missing = (~found_samples).sum()\n",
        "    print(f\"   ‚ö† {missing:,} samples had NaN values (filtered out)\")\n",
        "    \n",
        "    print(\"\\n   Missing by class:\")\n",
        "    for cls in np.unique(y):\n",
        "        cls_mask = (y == cls)\n",
        "        missing_cls = (~found_samples[cls_mask]).sum()\n",
        "        if missing_cls > 0:\n",
        "            print(f\"     Class {cls}: {missing_cls:,} / {cls_mask.sum():,}\")\n",
        "\n",
        "print(\"‚úÖ Extraction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Calculate Class Weights & Save\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINALIZING DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use only valid samples\n",
        "X_final = X[found_samples]\n",
        "y_final = y[found_samples]\n",
        "\n",
        "# Calculate class weights\n",
        "unique_classes, class_counts = np.unique(y_final, return_counts=True)\n",
        "class_weights = torch.zeros(6)\n",
        "\n",
        "for cls, count in zip(unique_classes, class_counts):\n",
        "    class_weights[cls] = 1.0 / count\n",
        "\n",
        "class_weights = class_weights / class_weights.sum() * 6\n",
        "\n",
        "print(\"\\nClass weights:\")\n",
        "for cls in range(6):\n",
        "    if cls in unique_classes:\n",
        "        print(f\"  Class {cls}: {class_weights[cls]:.4f}\")\n",
        "    else:\n",
        "        print(f\"  Class {cls}: MISSING ‚ùå\")\n",
        "\n",
        "# Save\n",
        "print(f\"\\nSaving to: {output_file}\")\n",
        "np.savez_compressed(\n",
        "    output_file,\n",
        "    X=X_final,\n",
        "    y=y_final,\n",
        "    class_weights=class_weights.numpy(),\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ DATASET CREATED SUCCESSFULLY!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nFile: wetland_dataset_1.5M_4Training.npz\")\n",
        "print(f\"Samples: {len(y_final):,}\")\n",
        "print(f\"Features: 64\")\n",
        "print(f\"Size: {X_final.nbytes / (1024**3):.2f} GB\")\n",
        "\n",
        "print(\"\\nFinal distribution:\")\n",
        "for cls, count in zip(unique_classes, class_counts):\n",
        "    pct = 100 * count / len(y_final)\n",
        "    print(f\"  Class {cls}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\nüéâ Ready to download and train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Verify (Optional)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VERIFICATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "data = np.load(output_file)\n",
        "\n",
        "print(f\"\\nArrays: {list(data.keys())}\")\n",
        "\n",
        "for key in data.keys():\n",
        "    arr = data[key]\n",
        "    print(f\"\\n{key}:\")\n",
        "    print(f\"  Shape: {arr.shape}\")\n",
        "    print(f\"  Type: {arr.dtype}\")\n",
        "    \n",
        "    if key == 'X':\n",
        "        print(f\"  Has NaN: {np.isnan(arr).any()} (should be False)\")\n",
        "        print(f\"  Has Inf: {np.isinf(arr).any()} (should be False)\")\n",
        "        print(f\"  Min: {arr.min():.4f}, Max: {arr.max():.4f}\")\n",
        "    elif key == 'y':\n",
        "        print(f\"  Classes: {np.unique(arr)}\")\n",
        "\n",
        "data.close()\n",
        "print(\"\\n‚úÖ Verification passed!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
