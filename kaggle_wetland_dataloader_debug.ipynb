{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Wetland Dataset Preparation - Tile-Optimized with Debug Logging\n",
                "\n",
                "This notebook extracts embeddings using tile-by-tile reading with extensive validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q rasterio tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import rasterio\n",
                "import numpy as np\n",
                "import torch\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "from collections import defaultdict\n",
                "import os\n",
                "\n",
                "# File paths for Kaggle\n",
                "KAGGLE_INPUT = '/kaggle/input/bo-river-and-google-earth'\n",
                "labels_file = f\"{KAGGLE_INPUT}/bow_river_wetlands_10m_final.tif\"\n",
                "embeddings_dir = Path(f\"{KAGGLE_INPUT}/Google_Dataset\")\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"WETLAND DATASET PREPARATION - TILE-OPTIMIZED\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Load Labels and Verify"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n[1/7] Loading labels...\")\n",
                "with rasterio.open(labels_file) as labels_src:\n",
                "    labels_full = labels_src.read(1)\n",
                "    print(f\"  ✓ Labels shape: {labels_full.shape}\")\n",
                "    print(f\"  ✓ Labels dtype: {labels_full.dtype}\")\n",
                "    print(f\"  ✓ Labels range: [{labels_full.min()}, {labels_full.max()}]\")\n",
                "    \n",
                "# Sanity check\n",
                "if labels_full.max() > 5 or labels_full.min() < 0:\n",
                "    print(\"  ⚠ WARNING: Labels outside expected range [0, 5]!\")\n",
                "else:\n",
                "    print(\"  ✓ Labels in valid range\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Find and Validate Tiles"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n[2/7] Finding embedding tiles...\")\n",
                "tile_files = sorted(embeddings_dir.glob(\"*.tif\"))\n",
                "print(f\"  ✓ Found {len(tile_files)} tile files\")\n",
                "\n",
                "if len(tile_files) == 0:\n",
                "    print(f\"  ⚠ ERROR: No tiles found in {embeddings_dir}!\")\n",
                "    print(f\"  Directory contents:\")\n",
                "    !ls -lh {embeddings_dir}\n",
                "else:\n",
                "    print(f\"  Sample filenames:\")\n",
                "    for tile in tile_files[:3]:\n",
                "        print(f\"    - {tile.name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Balanced Sampling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n[3/7] Balanced sampling...\")\n",
                "\n",
                "# Analyze class distribution\n",
                "valid_mask = (labels_full >= 0) & (labels_full <= 5)\n",
                "unique_classes, class_counts = np.unique(labels_full[valid_mask], return_counts=True)\n",
                "\n",
                "print(\"\\n  Class distribution in full dataset:\")\n",
                "for cls, count in zip(unique_classes, class_counts):\n",
                "    print(f\"    Class {cls}: {count:,} pixels ({100*count/valid_mask.sum():.2f}%)\")\n",
                "\n",
                "# Sampling strategy\n",
                "samples_per_class = {0: 600_000, 1: 19_225, 2: 150_000, 3: 500_000, 4: 150_000, 5: 100_000}\n",
                "print(f\"\\n  Target: {sum(samples_per_class.values()):,} balanced samples\")\n",
                "\n",
                "# Sample coordinates\n",
                "sampled_indices_y = []\n",
                "sampled_indices_x = []\n",
                "sampled_labels = []\n",
                "\n",
                "print(\"\\n  Sampling from each class:\")\n",
                "for cls in unique_classes:\n",
                "    class_mask = (labels_full == cls)\n",
                "    y_idx, x_idx = np.where(class_mask)\n",
                "    \n",
                "    n_available = len(y_idx)\n",
                "    n_target = samples_per_class[cls]\n",
                "    n_sample = min(n_target, n_available)\n",
                "    \n",
                "    if n_available > n_target:\n",
                "        sample_idx = np.random.choice(n_available, n_target, replace=False)\n",
                "    else:\n",
                "        sample_idx = np.arange(n_available)\n",
                "        print(f\"    ⚠ Class {cls}: only {n_available:,} available (target: {n_target:,})\")\n",
                "    \n",
                "    sampled_indices_y.append(y_idx[sample_idx])\n",
                "    sampled_indices_x.append(x_idx[sample_idx])\n",
                "    sampled_labels.append(np.full(n_sample, cls))\n",
                "    print(f\"    ✓ Class {cls}: sampled {n_sample:,}\")\n",
                "\n",
                "# Combine and shuffle\n",
                "y_indices = np.concatenate(sampled_indices_y)\n",
                "x_indices = np.concatenate(sampled_indices_x)\n",
                "y = np.concatenate(sampled_labels)\n",
                "\n",
                "np.random.seed(42)\n",
                "shuffle_idx = np.random.permutation(len(y_indices))\n",
                "y_indices = y_indices[shuffle_idx]\n",
                "x_indices = x_indices[shuffle_idx]\n",
                "y = y[shuffle_idx]\n",
                "\n",
                "print(f\"\\n  ✓ Total samples: {len(y):,}\")\n",
                "print(f\"  ✓ Coordinate ranges: y=[{y_indices.min()}, {y_indices.max()}], x=[{x_indices.min()}, {x_indices.max()}]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Calculate Class Weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n[4/7] Calculating class weights...\")\n",
                "unique_sampled, sampled_counts = np.unique(y, return_counts=True)\n",
                "class_weights = torch.zeros(6)\n",
                "for cls, count in zip(unique_sampled, sampled_counts):\n",
                "    class_weights[cls] = 1.0 / count\n",
                "class_weights = class_weights / class_weights.sum() * 6\n",
                "\n",
                "print(\"  Class weights for nn.CrossEntropyLoss:\")\n",
                "for cls in range(6):\n",
                "    print(f\"    Class {cls}: {class_weights[cls]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Extract Embeddings (Tile-by-Tile with Validation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n[5/7] Extracting embeddings from tiles...\")\n",
                "print(f\"  Processing {len(tile_files)} tiles\")\n",
                "\n",
                "# Pre-allocate\n",
                "n_samples = len(y_indices)\n",
                "X = np.zeros((n_samples, 64), dtype=np.float32)\n",
                "found_samples = np.zeros(n_samples, dtype=bool)\n",
                "\n",
                "print(f\"  ✓ Allocated array: {X.shape} ({X.nbytes / (1024**2):.1f} MB)\")\n",
                "\n",
                "# Process tiles\n",
                "tiles_processed = 0\n",
                "total_extracted = 0\n",
                "\n",
                "with tqdm(total=len(tile_files), desc=\"  Processing tiles\", unit=\" tiles\") as pbar:\n",
                "    for tile_idx, tile_file in enumerate(tile_files):\n",
                "        try:\n",
                "            with rasterio.open(tile_file) as tile_src:\n",
                "                # Read tile\n",
                "                tile_data = tile_src.read()\n",
                "                \n",
                "                # Parse coordinates from filename\n",
                "                parts = tile_file.stem.split('-')\n",
                "                if len(parts) == 3:\n",
                "                    tile_row_offset = int(parts[1])\n",
                "                    tile_col_offset = int(parts[2])\n",
                "                else:\n",
                "                    pbar.write(f\"    ⚠ Skipping {tile_file.name}: unexpected filename format\")\n",
                "                    continue\n",
                "                \n",
                "                # Find samples in this tile\n",
                "                tile_height, tile_width = tile_src.height, tile_src.width\n",
                "                in_tile_y = (y_indices >= tile_row_offset) & (y_indices < tile_row_offset + tile_height)\n",
                "                in_tile_x = (x_indices >= tile_col_offset) & (x_indices < tile_col_offset + tile_width)\n",
                "                in_tile_mask = in_tile_y & in_tile_x\n",
                "                \n",
                "                if in_tile_mask.any():\n",
                "                    # Get local coordinates\n",
                "                    local_y = y_indices[in_tile_mask] - tile_row_offset\n",
                "                    local_x = x_indices[in_tile_mask] - tile_col_offset\n",
                "                    \n",
                "                    # VALIDATION: Check coordinates are in bounds\n",
                "                    if (local_y < 0).any() or (local_y >= tile_height).any():\n",
                "                        pbar.write(f\"    ⚠ ERROR: Y coords out of bounds in {tile_file.name}!\")\n",
                "                        pbar.write(f\"      Tile height: {tile_height}, local_y range: [{local_y.min()}, {local_y.max()}]\")\n",
                "                        continue\n",
                "                    \n",
                "                    if (local_x < 0).any() or (local_x >= tile_width).any():\n",
                "                        pbar.write(f\"    ⚠ ERROR: X coords out of bounds in {tile_file.name}!\")\n",
                "                        pbar.write(f\"      Tile width: {tile_width}, local_x range: [{local_x.min()}, {local_x.max()}]\")\n",
                "                        continue\n",
                "                    \n",
                "                    # Extract embeddings\n",
                "                    for i, (ly, lx) in enumerate(zip(local_y, local_x)):\n",
                "                        global_idx = np.where(in_tile_mask)[0][i]\n",
                "                        X[global_idx, :] = tile_data[:, ly, lx]\n",
                "                        found_samples[global_idx] = True\n",
                "                    \n",
                "                    total_extracted += len(local_y)\n",
                "                    \n",
                "                    # VALIDATION: Check extracted data\n",
                "                    if tile_idx == 0:  # First tile - detailed check\n",
                "                        sample_values = tile_data[:, local_y[0], local_x[0]]\n",
                "                        pbar.write(f\"    ✓ First tile extraction check:\")\n",
                "                        pbar.write(f\"      Tile: {tile_file.name}\")\n",
                "                        pbar.write(f\"      Samples in tile: {len(local_y)}\")\n",
                "                        pbar.write(f\"      Sample data: {sample_values[:5]}... (first 5 of 64)\")\n",
                "                        if np.all(sample_values == 0) or np.all(np.isnan(sample_values)):\n",
                "                            pbar.write(f\"      ⚠⚠⚠ WARNING: Extracted values are all zero/NaN!\")\n",
                "\n",
                "                tiles_processed += 1\n",
                "        \n",
                "        except Exception as e:\n",
                "            pbar.write(f\"    ⚠ Error processing {tile_file.name}: {e}\")\n",
                "        \n",
                "        pbar.update(1)\n",
                "        pbar.set_postfix({\"extracted\": f\"{total_extracted:,}/{n_samples:,}\"})\n",
                "\n",
                "print(f\"\\n  ✓ Processed {tiles_processed}/{len(tile_files)} tiles\")\n",
                "print(f\"  ✓ Extracted {found_samples.sum():,} / {n_samples:,} samples ({100*found_samples.sum()/n_samples:.1f}%)\")\n",
                "\n",
                "if not found_samples.all():\n",
                "    print(f\"  ⚠ WARNING: {(~found_samples).sum():,} samples not found in tiles!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Validate Extracted Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n[6/7] Validating extracted embeddings...\")\n",
                "\n",
                "# Check for NaN/zeros\n",
                "nan_count = np.isnan(X).sum()\n",
                "zero_count = (X == 0).sum()\n",
                "total_values = X.size\n",
                "\n",
                "print(f\"  Data quality:\")\n",
                "print(f\"    NaN values: {nan_count:,} / {total_values:,} ({100*nan_count/total_values:.2f}%)\")\n",
                "print(f\"    Zero values: {zero_count:,} / {total_values:,} ({100*zero_count/total_values:.2f}%)\")\n",
                "print(f\"    Non-zero, non-NaN: {total_values - nan_count - zero_count:,} ({100*(total_values - nan_count - zero_count)/total_values:.2f}%)\")\n",
                "\n",
                "# Print sample values\n",
                "print(f\"\\n  Sample of first 3 rows:\")\n",
                "for i in range(min(3, len(X))):\n",
                "    print(f\"    Row {i}: [{X[i,0]:.3f}, {X[i,1]:.3f}, {X[i,2]:.3f}, ..., {X[i,-1]:.3f}]\")\n",
                "\n",
                "# Statistics\n",
                "if nan_count < total_values:\n",
                "    valid_X = X[~np.isnan(X)]\n",
                "    print(f\"\\n  Statistics (non-NaN values):\")\n",
                "    print(f\"    Min: {valid_X.min():.3f}\")\n",
                "    print(f\"    Max: {valid_X.max():.3f}\")\n",
                "    print(f\"    Mean: {valid_X.mean():.3f}\")\n",
                "    print(f\"    Std: {valid_X.std():.3f}\")\n",
                "\n",
                "# ERROR CHECK\n",
                "if nan_count > total_values * 0.99:\n",
                "    print(\"\\n  ⚠⚠⚠ CRITICAL ERROR: >99% of data is NaN! Extraction failed.\")\n",
                "elif zero_count > total_values * 0.99:\n",
                "    print(\"\\n  ⚠⚠⚠ CRITICAL ERROR: >99% of data is zero! Extraction failed.\")\n",
                "else:\n",
                "    print(\"\\n  ✓ Data appears valid!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Save Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n[7/7] Saving dataset...\")\n",
                "\n",
                "output_file = 'wetland_dataset_1.5M.npz'\n",
                "\n",
                "# Only save samples that were found\n",
                "X_final = X[found_samples]\n",
                "y_final = y[found_samples]\n",
                "\n",
                "np.savez_compressed(\n",
                "    output_file,\n",
                "    X=X_final,\n",
                "    y=y_final,\n",
                "    class_weights=class_weights.numpy(),\n",
                ")\n",
                "\n",
                "file_size = os.path.getsize(output_file)\n",
                "print(f\"  ✓ Saved: {output_file}\")\n",
                "print(f\"  ✓ File size: {file_size / (1024**2):.1f} MB\")\n",
                "print(f\"  ✓ Samples saved: {len(X_final):,}\")\n",
                "\n",
                "# VALIDATION: Reload and check\n",
                "print(\"\\n  Validating saved file...\")\n",
                "test_data = np.load(output_file)\n",
                "print(f\"    Keys: {test_data.files}\")\n",
                "print(f\"    X shape: {test_data['X'].shape}\")\n",
                "print(f\"    y shape: {test_data['y'].shape}\")\n",
                "print(f\"    Sample X values: {test_data['X'][0, :5]}\")\n",
                "\n",
                "if np.all(test_data['X'] == 0) or np.all(np.isnan(test_data['X'])):\n",
                "    print(\"\\n  ⚠⚠⚠ ERROR: Saved data is all zeros/NaN!\")\n",
                "else:\n",
                "    print(\"\\n  ✓ Saved data looks good!\")\n",
                "\n",
                "test_data.close()\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"COMPLETE!\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Download: {output_file}\")\n",
                "print(f\"Expected size: >100 MB (not 4-5 MB!)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
