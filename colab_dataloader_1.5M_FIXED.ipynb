{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# Wetland Dataset Creator - 1.5M Balanced Samples (FIXED)\n",
                "\n",
                "This notebook extracts **1.5 million balanced samples** from Google Earth Engine embedding tiles.\n",
                "\n",
                "**Key Features:**\n",
                "- âœ… **Memory-efficient** - processes labels in chunks\n",
                "- âœ… Includes **Class 0** (background)\n",
                "- âœ… Filters **only NaN values**\n",
                "- âœ… Balanced sampling across all 6 classes\n",
                "- âœ… Tile-optimized for speed\n",
                "\n",
                "**Input:** Clean embedding tiles in `MyDrive/EarthEngine`\n",
                "\n",
                "**Output:** `wetland_dataset_1.5M_REDO.npz`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup"
            },
            "outputs": [],
            "source": [
                "# CELL 1: SETUP & MOUNT GOOGLE DRIVE\n",
                "print(\"ðŸš€ Setting up environment...\")\n",
                "\n",
                "import os\n",
                "import sys\n",
                "from google.colab import drive\n",
                "\n",
                "# Mount Google Drive\n",
                "if not os.path.exists('/content/drive'):\n",
                "    print(\"ðŸ“‚ Mounting Google Drive...\")\n",
                "    drive.mount('/content/drive')\n",
                "else:\n",
                "    print(\"âœ“ Google Drive already mounted\")\n",
                "\n",
                "# Install rasterio if needed\n",
                "try:\n",
                "    import rasterio\n",
                "    print(\"âœ“ rasterio already installed\")\n",
                "except ImportError:\n",
                "    print(\"ðŸ“¦ Installing rasterio...\")\n",
                "    !pip install -q rasterio\n",
                "    import rasterio\n",
                "\n",
                "# Install tqdm for progress bars\n",
                "try:\n",
                "    from tqdm import tqdm\n",
                "   print(\"âœ“ tqdm already installed\")\n",
                "except ImportError:\n",
                "    print(\"ðŸ“¦ Installing tqdm...\")\n",
                "    !pip install -q tqdm\n",
                "    from tqdm import tqdm\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "from pathlib import Path\n",
                "from collections import defaultdict\n",
                "\n",
                "print(\"\\nâœ… Setup complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "config"
            },
            "outputs": [],
            "source": [
                "# CELL 2: CONFIGURATION\n",
                "print(\"=\"*60)\n",
                "print(\"CONFIGURATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# File paths\n",
                "labels_file = \"/content/drive/MyDrive/bow_river_wetlands_10m_final.tif\"\n",
                "embeddings_dir = Path(\"/content/drive/MyDrive/EarthEngine\")\n",
                "output_file = '/content/drive/MyDrive/wetland_dataset_1.5M_REDO.npz'\n",
                "\n",
                "# Balanced sampling strategy - INCLUDING CLASS 0!\n",
                "samples_per_class = {\n",
                "    0: 600_000,   # Background - INCLUDED!\n",
                "    1: 19_225,    # Rare wetland type\n",
                "    2: 150_000,\n",
                "    3: 500_000,\n",
                "    4: 150_000,\n",
                "    5: 100_000,\n",
                "}\n",
                "\n",
                "total_target = sum(samples_per_class.values())\n",
                "\n",
                "print(f\"\\nLabels file: {labels_file}\")\n",
                "print(f\"Embeddings dir: {embeddings_dir}\")\n",
                "print(f\"Output file: {output_file}\")\n",
                "print(f\"\\nTarget samples: {total_target:,}\")\n",
                "print(f\"\\nSamples per class:\")\n",
                "for cls, count in samples_per_class.items():\n",
                "    print(f\"  Class {cls}: {count:,}\")\n",
                "\n",
                "# Verify files exist\n",
                "assert os.path.exists(labels_file), f\"âŒ Labels file not found: {labels_file}\"\n",
                "assert embeddings_dir.exists(), f\"âŒ Embeddings directory not found: {embeddings_dir}\"\n",
                "\n",
                "print(\"\\nâœ… Configuration validated!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "metadata"
            },
            "outputs": [],
            "source": [
                "# CELL 3: GET BASIC INFO (WITHOUT LOADING FULL ARRAY)\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ANALYZING LABELS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Get metadata WITHOUT loading the full array\n",
                "with rasterio.open(labels_file) as src:\n",
                "    labels_shape = (src.height, src.width)\n",
                "    labels_crs = src.crs\n",
                "    labels_bounds = src.bounds\n",
                "    labels_transform = src.transform\n",
                "    \n",
                "    print(f\"\\nLabels shape: {labels_shape}\")\n",
                "    print(f\"Labels CRS: {labels_crs}\")\n",
                "    print(f\"Labels bounds: {labels_bounds}\")\n",
                "\n",
                "# Get list of embedding tiles\n",
                "tile_files = sorted(embeddings_dir.glob(\"*.tif\"))\n",
                "print(f\"\\nFound {len(tile_files)} embedding tiles\")\n",
                "\n",
                "print(\"\\nâœ… Metadata loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "sample_coords"
            },
            "outputs": [],
            "source": [
                "# CELL 4: MEMORY-EFFICIENT SAMPLING (Process in chunks) - FIXED WINDOWS\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"SAMPLING PIXEL COORDINATES (MEMORY-EFFICIENT)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Initialize storage for sampled coordinates\n",
                "sampled_coords = {cls: {'y': [], 'x': []} for cls in samples_per_class.keys()}\n",
                "samples_collected = {cls: 0 for cls in samples_per_class.keys()}\n",
                "\n",
                "np.random.seed(42)\n",
                "\n",
                "# Process labels in chunks (window-by-window)\n",
                "print(\"\\nScanning labels in chunks...\")\n",
                "with rasterio.open(labels_file) as src:\n",
                "    # FIXED: block_windows returns (block_id, Window) tuples\n",
                "    windows = list(src.block_windows(1))\n",
                "    \n",
                "    # Shuffle windows for random sampling\n",
                "    np.random.shuffle(windows)\n",
                "    \n",
                "    # FIXED: Unpack both block_id and window from tuple\n",
                "    for idx, (block_id, window) in tqdm(enumerate(windows), total=len(windows), desc=\"Scanning blocks\"):\n",
                "        # Read this chunk of labels\n",
                "        labels_chunk = src.read(1, window=window)\n",
                "        \n",
                "        # Get window offset\n",
                "        row_off = window.row_off\n",
                "        col_off = window.col_off\n",
                "        \n",
                "        # For each class, sample from this chunk\n",
                "        for cls in samples_per_class.keys():\n",
                "            # Check if we still need samples for this class\n",
                "            if samples_collected[cls] >= samples_per_class[cls]:\n",
                "                continue\n",
                "            \n",
                "            # Find pixels of this class in the chunk\n",
                "            class_mask = (labels_chunk == cls)\n",
                "            y_local, x_local = np.where(class_mask)\n",
                "            \n",
                "            if len(y_local) == 0:\n",
                "                continue\n",
                "            \n",
                " # Convert to global coordinates\n",
                "            y_global = y_local + row_off\n",
                "            x_global = x_local + col_off\n",
                "            \n",
                "            # How many more do we need?\n",
                "            needed = samples_per_class[cls] - samples_collected[cls]\n",
                "            available = len(y_local)\n",
                "            \n",
                "            # Sample what we can from this chunk\n",
                "            n_sample = min(needed, available)\n",
                "            \n",
                "            if available > needed:\n",
                "                # Randomly select\n",
                "                sample_idx = np.random.choice(available, n_sample, replace=False)\n",
                "                sampled_coords[cls]['y'].append(y_global[sample_idx])\n",
                "                sampled_coords[cls]['x'].append(x_global[sample_idx])\n",
                "            else:\n",
                "                # Take all\n",
                "                sampled_coords[cls]['y'].append(y_global)\n",
                "                sampled_coords[cls]['x'].append(x_global)\n",
                "            \n",
                "            samples_collected[cls] += n_sample\n",
                "        \n",
                "        # Check if we're done with all classes\n",
                "        if all(samples_collected[cls] >= samples_per_class[cls] for cls in samples_per_class.keys()):\n",
                "            print(f\"\\nâœ“ Collected all required samples after {idx+1}/{len(windows)} blocks\")\n",
                "            break\n",
                "\n",
                "# Combine coordinates for each class\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"COMBINING COORDINATES\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "all_y = []\n",
                "all_x = []\n",
                "all_labels = []\n",
                "\n",
                "for cls in samples_per_class.keys():\n",
                "    if len(sampled_coords[cls]['y']) > 0:\n",
                "        y_coords = np.concatenate(sampled_coords[cls]['y'])\n",
                "        x_coords = np.concatenate(sampled_coords[cls]['x'])\n",
                "        \n",
                "        # Trim to exact target if we collected more\n",
                "        if len(y_coords) > samples_per_class[cls]:\n",
                "            y_coords = y_coords[:samples_per_class[cls]]\n",
                "            x_coords = x_coords[:samples_per_class[cls]]\n",
                "        \n",
                "        all_y.append(y_coords)\n",
                "        all_x.append(x_coords)\n",
                "        all_labels.append(np.full(len(y_coords), cls))\n",
                "        \n",
                "        print(f\"  Class {cls}: {len(y_coords):,} samples\")\n",
                "\n",
                "# Combine all classes\n",
                "y_indices = np.concatenate(all_y)\n",
                "x_indices = np.concatenate(all_x)\n",
                "y = np.concatenate(all_labels)\n",
                "\n",
                "# Shuffle\n",
                "shuffle_idx = np.random.permutation(len(y_indices))\n",
                "y_indices = y_indices[shuffle_idx]\n",
                "x_indices = x_indices[shuffle_idx]\n",
                "y = y[shuffle_idx]\n",
                "\n",
                "print(f\"\\nTotal sampled coordinates: {len(y):,}\")\n",
                "print(\"\\nâœ… Pixel coordinates sampled!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "extract_embeddings"
            },
            "outputs": [],
            "source": [
                "# CELL 5: EXTRACT EMBEDDINGS (TILE-BY-TILE)\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"EXTRACTING EMBEDDINGS - TILE-BY-TILE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nWill process {len(tile_files)} tiles\\n\")\n",
                "\n",
                "# Pre-allocate output\n",
                "n_samples = len(y_indices)\n",
                "X = np.zeros((n_samples, 64), dtype=np.float32)  # 64 bands\n",
                "found_samples = np.zeros(n_samples, dtype=bool)\n",
                "\n",
                "# Process each tile\n",
                "with tqdm(total=len(tile_files), desc=\"Processing tiles\", unit=\" tiles\") as pbar:\n",
                "    for tile_file in tile_files:\n",
                "        # Open tile\n",
                "        with rasterio.open(tile_file) as tile_src:\n",
                "            # Get tile position from filename\n",
                "            # Format: bow_river_embeddings_2020_CORRECTED-RRRRRRRRRR-CCCCCCCCCC.tif\n",
                "            parts = tile_file.stem.split('-')\n",
                "            if len(parts) >= 3:\n",
                "                try:\n",
                "                    tile_row_offset = int(parts[-2])\n",
                "                    tile_col_offset = int(parts[-1])\n",
                "                except ValueError:\n",
                "                    pbar.update(1)\n",
                "                    continue\n",
                "            else:\n",
                "                pbar.update(1)\n",
                "                continue\n",
                "            \n",
                "            # Get tile dimensions\n",
                "            tile_height, tile_width = tile_src.height, tile_src.width\n",
                "            \n",
                "            # Find which samples fall within this tile\n",
                "            in_tile_y = (y_indices >= tile_row_offset) & (y_indices < tile_row_offset + tile_height)\n",
                "            in_tile_x = (x_indices >= tile_col_offset) & (x_indices < tile_col_offset + tile_width)\n",
                "            in_tile_mask = in_tile_y & in_tile_x\n",
                "            \n",
                "            if in_tile_mask.any():\n",
                "                # Read tile data\n",
                "                tile_data = tile_src.read()  # Shape: (64, height, width)\n",
                "                \n",
                "                # Get local coordinates within this tile\n",
                "                local_y = y_indices[in_tile_mask] - tile_row_offset\n",
                "                local_x = x_indices[in_tile_mask] - tile_col_offset\n",
                "                \n",
                "                # Extract embeddings for these samples\n",
                "                for i, (ly, lx) in enumerate(zip(local_y, local_x)):\n",
                "                    global_idx = np.where(in_tile_mask)[0][i]\n",
                "                    pixel_values = tile_data[:, ly, lx]\n",
                "                    \n",
                "                    # ONLY filter NaN values - keep everything else!\n",
                "                    if not np.isnan(pixel_values).any():\n",
                "                        X[global_idx, :] = pixel_values\n",
                "                        found_samples[global_idx] = True\n",
                "        \n",
                "        pbar.update(1)\n",
                "        pbar.set_postfix({\"found\": f\"{found_samples.sum():,}/{n_samples:,}\"})\n",
                "\n",
                "print(f\"\\nâœ“ Extracted {found_samples.sum():,} / {n_samples:,} samples\")\n",
                "\n",
                "if not found_samples.all():\n",
                "    missing = (~found_samples).sum()\n",
                "    print(f\"   âš  Warning: {missing:,} samples not found (may have NaN values)\")\n",
                "    \n",
                "    # Show which classes were affected\n",
                "    print(\"\\n   Missing samples by class:\")\n",
                "    unique_classes = np.unique(y)\n",
                "    for cls in unique_classes:\n",
                "        cls_mask = (y == cls)\n",
                "        missing_in_class = (~found_samples[cls_mask]).sum()\n",
                "        total_in_class = cls_mask.sum()\n",
                "        if missing_in_class > 0:\n",
                "            print(f\"     Class {cls}: {missing_in_class:,} / {total_in_class:,} missing\")\n",
                "\n",
                "print(\"\\nâœ… Embedding extraction complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "class_weights"
            },
            "outputs": [],
            "source": [
                "# CELL 6: CALCULATE CLASS WEIGHTS\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"CALCULATING CLASS WEIGHTS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Only use samples we successfully found\n",
                "y_valid = y[found_samples]\n",
                "\n",
                "# Calculate class weights for balanced training\n",
                "unique_sampled, sampled_counts = np.unique(y_valid, return_counts=True)\n",
                "class_weights = torch.zeros(6)\n",
                "\n",
                "for cls, count in zip(unique_sampled, sampled_counts):\n",
                "    class_weights[cls] = 1.0 / count\n",
                "\n",
                "# Normalize so they sum to 6 (number of classes)\n",
                "class_weights = class_weights / class_weights.sum() * 6\n",
                "\n",
                "print(\"\\nClass weights for training:\")\n",
                "for cls in range(6):\n",
                "    if cls in unique_sampled:\n",
                "        print(f\"   Class {cls}: {class_weights[cls]:.4f}\")\n",
                "    else:\n",
                "        print(f\"   Class {cls}: MISSING\")\n",
                "\n",
                "print(\"\\nâœ… Class weights calculated!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_dataset"
            },
            "outputs": [],
            "source": [
                "# CELL 7: SAVE DATASET\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"SAVING DATASET\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "print(f\"\\nSaving to: {output_file}\")\n",
                "\n",
                "# Save only valid samples (no NaN)\n",
                "np.savez_compressed(\n",
                "    output_file,\n",
                "    X=X[found_samples],\n",
                "    y=y[found_samples],\n",
                "    class_weights=class_weights.numpy(),\n",
                ")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"âœ… DATASET CREATION COMPLETE!\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"\\nDataset: {output_file}\")\n",
                "print(f\"Samples: {found_samples.sum():,}\")\n",
                "print(f\"Features: 64\")\n",
                "print(f\"Size: {X[found_samples].nbytes / (1024**3):.2f} GB in memory\")\n",
                "print(f\"\\nFinal class distribution:\")\n",
                "for cls, count in zip(unique_sampled, sampled_counts):\n",
                "    pct = 100 * count / found_samples.sum()\n",
                "    print(f\"  Class {cls}: {count:,} samples ({pct:.2f}%)\")\n",
                "print(f\"\\nUse in PyTorch: nn.CrossEntropyLoss(weight=class_weights)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "verify"
            },
            "outputs": [],
            "source": [
                "# CELL 8: VERIFY SAVED FILE\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"VERIFICATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Load and verify the saved file\n",
                "data = np.load(output_file)\n",
                "\n",
                "print(f\"\\nLoaded from: {output_file}\")\n",
                "print(f\"\\nArrays in file: {list(data.keys())}\")\n",
                "\n",
                "for key in data.keys():\n",
                "    arr = data[key]\n",
                "    print(f\"\\n{key}:\")\n",
                "    print(f\"  Shape: {arr.shape}\")\n",
                "    print(f\"  Type: {arr.dtype}\")\n",
                "    \n",
                "    if key == 'X':\n",
                "        print(f\"  Has NaN: {np.isnan(arr).any()}\")\n",
                "        print(f\"  Has Inf: {np.isinf(arr).any()}\")\n",
                "        print(f\"  Min: {arr.min():.4f}, Max: {arr.max():.4f}\")\n",
                "    elif key == 'y':\n",
                "        unique, counts = np.unique(arr, return_counts=True)\n",
                "        print(f\"  Classes: {unique}\")\n",
                "\n",
                "data.close()\n",
                "\n",
                "print(\"\\nâœ… Verification complete!\")\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ðŸŽ‰ READY FOR MODEL TRAINING!\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
