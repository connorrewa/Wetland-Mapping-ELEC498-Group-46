{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Wetland Mapping - Dataset Preparation\n",
                "This notebook prepares the balanced dataset for wetland classification using Google Earth Engine embeddings.\n",
                "\n",
                "## Kaggle Dataset Setup:\n",
                "- **Dataset name**: `Bo_River_and_Google_Earth`\n",
                "- **File structure**:\n",
                "  - `bow_river_wetlands_10m_final.tif` (labels)\n",
                "  - `Google_Dataset/` (folder with 77 embedding TIF tiles)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q rasterio tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone your repository (gee_embed_CNN_dev branch)\n",
                "!git clone -b gee_embed_CNN_dev https://github.com/Jcub05/Wetland-Mapping-ELEC498-Group-46.git\n",
                "%cd Wetland-Mapping-ELEC498-Group-46"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Link Kaggle Dataset Files\n",
                "Create symbolic links to avoid copying 5GB of data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Your Kaggle dataset (Kaggle converts underscores to hyphens in URLs)\n",
                "KAGGLE_INPUT = '/kaggle/input/bo-river-and-google-earth'\n",
                "\n",
                "# Verify dataset exists\n",
                "if not os.path.exists(KAGGLE_INPUT):\n",
                "    print(f\"âš  Error: Dataset not found at {KAGGLE_INPUT}\")\n",
                "    print(\"Available datasets:\")\n",
                "    !ls -la /kaggle/input/\n",
                "else:\n",
                "    print(f\"âœ“ Dataset found: {KAGGLE_INPUT}\")\n",
                "    print(\"\\nContents:\")\n",
                "    !ls -lh {KAGGLE_INPUT}\n",
                "    \n",
                "# Create symbolic links (no copying needed!)\n",
                "print(\"\\nLinking dataset files...\")\n",
                "if not os.path.exists('Google_Dataset'):\n",
                "    os.symlink(os.path.join(KAGGLE_INPUT, 'Google_Dataset'), 'Google_Dataset')\n",
                "    print(\"  âœ“ Linked Google_Dataset folder\")\n",
                "\n",
                "if not os.path.exists('bow_river_wetlands_10m_final.tif'):\n",
                "    os.symlink(\n",
                "        os.path.join(KAGGLE_INPUT, 'bow_river_wetlands_10m_final.tif'),\n",
                "        'bow_river_wetlands_10m_final.tif'\n",
                "    )\n",
                "    print(\"  âœ“ Linked labels file\")\n",
                "\n",
                "print(\"\\nâœ“ All files ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Build VRT (Virtual Raster)\n",
                "Combine all TIF tiles into a single virtual raster"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the VRT builder\n",
                "!python build_vrt_and_verify.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Load and Analyze Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import rasterio\n",
                "import numpy as np\n",
                "import torch\n",
                "from tqdm import tqdm\n",
                "from collections import defaultdict\n",
                "\n",
                "# File paths\n",
                "embeddings_file = \"bow_river_embeddings_2020_matched.vrt\"\n",
                "labels_file = \"bow_river_wetlands_10m_final.tif\"\n",
                "\n",
                "print(\"Loading labels...\")\n",
                "with rasterio.open(labels_file) as labels_src:\n",
                "    labels_full = labels_src.read(1)\n",
                "    print(f\"Labels (original): {labels_full.shape}\")\n",
                "\n",
                "print(f\"\\nOpening embeddings VRT: {embeddings_file}\")\n",
                "embeddings_src = rasterio.open(embeddings_file)\n",
                "print(f\"Embeddings: {embeddings_src.count} bands x {embeddings_src.height} x {embeddings_src.width}\")\n",
                "\n",
                "# Crop labels to match embeddings\n",
                "labels = labels_full[:embeddings_src.height, :embeddings_src.width]\n",
                "print(f\"Labels (cropped): {labels.shape}\")\n",
                "\n",
                "# Verify dimensions match\n",
                "assert (embeddings_src.height, embeddings_src.width) == labels.shape, \"Dimension mismatch!\"\n",
                "print(\"âœ“ Dimensions match!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze class distribution\n",
                "valid_mask = (labels >= 0) & (labels <= 5)\n",
                "valid_count = valid_mask.sum()\n",
                "print(f\"\\nTotal labeled pixels: {valid_count:,} out of {labels.size:,} ({100*valid_count/labels.size:.2f}%)\")\n",
                "\n",
                "unique_classes, class_counts = np.unique(labels[valid_mask], return_counts=True)\n",
                "print(\"\\nClass distribution:\")\n",
                "for cls, count in zip(unique_classes, class_counts):\n",
                "    print(f\"  Class {cls}: {count:,} pixels ({100*count/valid_count:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Balanced Sampling (~1.5M samples)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Balanced sampling strategy\n",
                "samples_per_class = {\n",
                "    0: 600_000,   # Background: plenty available, need good \"not wetland\" examples\n",
                "    1: 19_225,    # Class 1: USE ALL (smallest class - only 19K available)\n",
                "    2: 150_000,   # Class 2: moderate wetland type\n",
                "    3: 500_000,   # Class 3: largest wetland class, get lots of variety  \n",
                "    4: 150_000,   # Class 4: moderate wetland type\n",
                "    5: 100_000,   # Class 5: moderate wetland type\n",
                "}\n",
                "total_target = sum(samples_per_class.values())\n",
                "print(f\"Balanced sampling strategy (target: {total_target:,} samples)\\n\")\n",
                "\n",
                "sampled_indices_y = []\n",
                "sampled_indices_x = []\n",
                "sampled_labels = []\n",
                "\n",
                "print(\"Sampling pixels from each class...\")\n",
                "for cls in unique_classes:\n",
                "    class_mask = (labels == cls)\n",
                "    y_idx, x_idx = np.where(class_mask)\n",
                "    \n",
                "    n_available = len(y_idx)\n",
                "    n_target = samples_per_class[cls]\n",
                "    n_sample = min(n_target, n_available)\n",
                "    \n",
                "    # Random sampling\n",
                "    if n_available > n_target:\n",
                "        sample_idx = np.random.choice(n_available, n_target, replace=False)\n",
                "    else:\n",
                "        sample_idx = np.arange(n_available)\n",
                "        print(f\"  âš  Class {cls}: only {n_available:,} available (target: {n_target:,})\")\n",
                "    \n",
                "    sampled_indices_y.append(y_idx[sample_idx])\n",
                "    sampled_indices_x.append(x_idx[sample_idx])\n",
                "    sampled_labels.append(np.full(n_sample, cls))\n",
                "    \n",
                "    print(f\"  Class {cls}: sampled {n_sample:,} / {n_available:,} pixels\")\n",
                "\n",
                "# Combine and shuffle\n",
                "y_indices = np.concatenate(sampled_indices_y)\n",
                "x_indices = np.concatenate(sampled_indices_x)\n",
                "y = np.concatenate(sampled_labels)\n",
                "\n",
                "np.random.seed(42)  # For reproducibility\n",
                "shuffle_idx = np.random.permutation(len(y_indices))\n",
                "y_indices = y_indices[shuffle_idx]\n",
                "x_indices = x_indices[shuffle_idx]\n",
                "y = y[shuffle_idx]\n",
                "\n",
                "print(f\"\\nTotal balanced samples: {len(y):,}\")\n",
                "unique_sampled, sampled_counts = np.unique(y, return_counts=True)\n",
                "print(\"\\nSampled distribution:\")\n",
                "for cls, count in zip(unique_sampled, sampled_counts):\n",
                "    print(f\"  Class {cls}: {count:,} samples ({100*count/len(y):.2f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate class weights for loss function\n",
                "class_weights = torch.zeros(6)\n",
                "for cls, count in zip(unique_sampled, sampled_counts):\n",
                "    class_weights[cls] = 1.0 / count\n",
                "class_weights = class_weights / class_weights.sum() * 6  # Normalize\n",
                "\n",
                "print(\"Class weights for loss function:\")\n",
                "for cls in range(6):\n",
                "    print(f\"  Class {cls}: {class_weights[cls]:.4f}\")\n",
                "print(\"\\nðŸ’¡ Use in training: nn.CrossEntropyLoss(weight=class_weights)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Extract Embeddings (Optimized Batch Reading)\n",
                "**This is the slow part** - reading 1.5M pixels from disk.  \n",
                "Optimized to read entire rows at once (~100x faster than pixel-by-pixel).\n",
                "\n",
                "**Expected time: 10-20 minutes**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract embeddings using row-batched reading\n",
                "print(\"Reading embeddings for sampled pixels (optimized batching)...\")\n",
                "n_samples = len(y_indices)\n",
                "X = np.zeros((n_samples, embeddings_src.count), dtype=np.float32)\n",
                "\n",
                "# Group samples by row for efficient batch reading\n",
                "row_to_samples = defaultdict(list)\n",
                "for idx, (y_coord, x_coord) in enumerate(zip(y_indices, x_indices)):\n",
                "    row_to_samples[y_coord].append((idx, x_coord))\n",
                "\n",
                "print(f\"Grouped {n_samples:,} samples into {len(row_to_samples):,} unique rows\")\n",
                "print(\"This will process rows in batches...\\n\")\n",
                "\n",
                "# Read row by row\n",
                "sample_count = 0\n",
                "with tqdm(total=len(row_to_samples), desc=\"Reading rows\", unit=\" rows\") as pbar:\n",
                "    for row_idx in sorted(row_to_samples.keys()):\n",
                "        # Read entire row at once (64 bands x 31,428 pixels)\n",
                "        row_data = embeddings_src.read(window=((row_idx, row_idx+1), (0, embeddings_src.width)))\n",
                "        row_data = row_data[:, 0, :]  # Shape: (64, width)\n",
                "        \n",
                "        # Extract sampled pixels from this row\n",
                "        for sample_idx, col_idx in row_to_samples[row_idx]:\n",
                "            X[sample_idx, :] = row_data[:, col_idx]\n",
                "            sample_count += 1\n",
                "        \n",
                "        pbar.update(1)\n",
                "\n",
                "embeddings_src.close()\n",
                "print(f\"\\nâœ“ Successfully loaded {sample_count:,} samples\")\n",
                "print(f\"  X shape: {X.shape}\")\n",
                "print(f\"  Memory: {X.nbytes / (1024**3):.2f} GB\")\n",
                "print(f\"  y shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Save Preprocessed Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save dataset as compressed .npz file\n",
                "output_file = 'wetland_dataset_1.5M.npz'\n",
                "np.savez_compressed(\n",
                "    output_file,\n",
                "    X=X,\n",
                "    y=y,\n",
                "    class_weights=class_weights.numpy(),\n",
                "    samples_per_class=np.array(list(samples_per_class.values()))\n",
                ")\n",
                "\n",
                "import os\n",
                "file_size_gb = os.path.getsize(output_file) / (1024**3)\n",
                "print(f\"âœ“ Dataset saved to: {output_file}\")\n",
                "print(f\"  File size: {file_size_gb:.2f} GB\")\n",
                "print(f\"\\nðŸ“¥ Download this file to use for training!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Verify Saved Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and verify\n",
                "data = np.load(output_file)\n",
                "print(\"Dataset contents:\")\n",
                "for key in data.files:\n",
                "    print(f\"  {key}: {data[key].shape}\")\n",
                "\n",
                "print(f\"\\nâœ“ Final class distribution:\")\n",
                "unique, counts = np.unique(data['y'], return_counts=True)\n",
                "for cls, count in zip(unique, counts):\n",
                "    print(f\"  Class {cls}: {count:,} samples ({100*count/len(data['y']):.2f}%)\")\n",
                "\n",
                "print(f\"\\n\\nTo use in training:\")\n",
                "print(\"```python\")\n",
                "print(\"data = np.load('wetland_dataset_1.5M.npz')\")\n",
                "print(\"X, y = data['X'], data['y']\")\n",
                "print(\"class_weights = torch.from_numpy(data['class_weights'])\")\n",
                "print(\"```\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
