{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wetland Dataset Creator - 1.5M Balanced Samples\n",
        "\n",
        "This notebook extracts **1.5 million balanced samples** from Google Earth Engine embedding tiles.\n",
        "\n",
        "**Key Features:**\n",
        "- âœ… Includes **Class 0** (background)\n",
        "- âœ… Filters **only NaN values**\n",
        "- âœ… Balanced sampling across all 6 classes\n",
        "- âœ… Tile-optimized for speed\n",
        "\n",
        "**Input:** Clean embedding tiles in `MyDrive/EarthEngine`\n",
        "\n",
        "**Output:** `wetland_dataset_1.5M_REDO.npz`"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: SETUP & MOUNT GOOGLE DRIVE\n",
        "print(\"ðŸš€ Setting up environment...\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"ðŸ“‚ Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"âœ“ Google Drive already mounted\")\n",
        "\n",
        "# Install rasterio if needed\n",
        "try:\n",
        "    import rasterio\n",
        "    print(\"âœ“ rasterio already installed\")\n",
        "except ImportError:\n",
        "    print(\"ðŸ“¦ Installing rasterio...\")\n",
        "    !pip install -q rasterio\n",
        "    import rasterio\n",
        "\n",
        "# Install tqdm for progress bars\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "    print(\"âœ“ tqdm already installed\")\n",
        "except ImportError:\n",
        "    print(\"ðŸ“¦ Installing tqdm...\")\n",
        "    !pip install -q tqdm\n",
        "    from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"\\nâœ… Setup complete!\")"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: CONFIGURATION\n",
        "print(\"=\"*60)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# File paths\n",
        "labels_file = \"/content/drive/MyDrive/bow_river_wetlands_10m_final.tif\"\n",
        "embeddings_dir = Path(\"/content/drive/MyDrive/EarthEngine\")\n",
        "output_file = '/content/drive/MyDrive/wetland_dataset_1.5M_REDO.npz'\n",
        "\n",
        "# Balanced sampling strategy - INCLUDING CLASS 0!\n",
        "samples_per_class = {\n",
        "    0: 600_000,   # Background - INCLUDED!\n",
        "    1: 19_225,    # Rare wetland type\n",
        "    2: 150_000,\n",
        "    3: 500_000,\n",
        "    4: 150_000,\n",
        "    5: 100_000,\n",
        "}\n",
        "\n",
        "total_target = sum(samples_per_class.values())\n",
        "\n",
        "print(f\"\\nLabels file: {labels_file}\")\n",
        "print(f\"Embeddings dir: {embeddings_dir}\")\n",
        "print(f\"Output file: {output_file}\")\n",
        "print(f\"\\nTarget samples: {total_target:,}\")\n",
        "print(f\"\\nSamples per class:\")\n",
        "for cls, count in samples_per_class.items():\n",
        "    print(f\"  Class {cls}: {count:,}\")\n",
        "\n",
        "# Verify files exist\n",
        "assert os.path.exists(labels_file), f\"âŒ Labels file not found: {labels_file}\"\n",
        "assert embeddings_dir.exists(), f\"âŒ Embeddings directory not found: {embeddings_dir}\"\n",
        "\n",
        "print(\"\\nâœ… Configuration validated!\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: LOAD LABELS & ANALYZE\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING LABELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n1. Loading labels...\")\n",
        "with rasterio.open(labels_file) as labels_src:\n",
        "    labels_full = labels_src.read(1)\n",
        "    print(f\"   Labels shape: {labels_full.shape}\")\n",
        "    print(f\"   Labels CRS: {labels_src.crs}\")\n",
        "    print(f\"   Labels bounds: {labels_src.bounds}\")\n",
        "\n",
        "# Get list of all embedding tiles\n",
        "tile_files = sorted(embeddings_dir.glob(\"*.tif\"))\n",
        "print(f\"\\n2. Found {len(tile_files)} embedding tiles\")\n",
        "\n",
        "# Analyze class distribution\n",
        "valid_mask = (labels_full >= 0) & (labels_full <= 5)\n",
        "unique_classes, class_counts = np.unique(labels_full[valid_mask], return_counts=True)\n",
        "print(\"\\n3. Class distribution in labels:\")\n",
        "for cls, count in zip(unique_classes, class_counts):\n",
        "    print(f\"   Class {cls}: {count:,} pixels ({100*count/valid_mask.sum():.2f}%)\")\n",
        "\n",
        "print(\"\\nâœ… Labels loaded successfully!\")"
      ],
      "metadata": {
        "id": "load_labels"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: SAMPLE PIXEL COORDINATES (BALANCED)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAMPLING PIXEL COORDINATES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sampled_indices_y = []\n",
        "sampled_indices_x = []\n",
        "sampled_labels = []\n",
        "\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "for cls in unique_classes:\n",
        "    # Find all pixels of this class\n",
        "    class_mask = (labels_full == cls)\n",
        "    y_idx, x_idx = np.where(class_mask)\n",
        "    \n",
        "    n_available = len(y_idx)\n",
        "    n_target = samples_per_class[cls]\n",
        "    n_sample = min(n_target, n_available)\n",
        "    \n",
        "    # Sample randomly\n",
        "    if n_available > n_target:\n",
        "        sample_idx = np.random.choice(n_available, n_target, replace=False)\n",
        "    else:\n",
        "        sample_idx = np.arange(n_available)\n",
        "    \n",
        "    sampled_indices_y.append(y_idx[sample_idx])\n",
        "    sampled_indices_x.append(x_idx[sample_idx])\n",
        "    sampled_labels.append(np.full(n_sample, cls))\n",
        "    \n",
        "    print(f\"   Class {cls}: sampled {n_sample:,} / {n_available:,}\")\n",
        "\n",
        "# Combine and shuffle\n",
        "y_indices = np.concatenate(sampled_indices_y)\n",
        "x_indices = np.concatenate(sampled_indices_x)\n",
        "y = np.concatenate(sampled_labels)\n",
        "\n",
        "shuffle_idx = np.random.permutation(len(y_indices))\n",
        "y_indices = y_indices[shuffle_idx]\n",
        "x_indices = x_indices[shuffle_idx]\n",
        "y = y[shuffle_idx]\n",
        "\n",
        "print(f\"\\n   Total sampled coordinates: {len(y):,}\")\n",
        "print(\"\\nâœ… Pixel coordinates sampled!\")"
      ],
      "metadata": {
        "id": "sample_coords"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: EXTRACT EMBEDDINGS (TILE-BY-TILE)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXTRACTING EMBEDDINGS - TILE-BY-TILE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nWill process {len(tile_files)} tiles\\n\")\n",
        "\n",
        "# Pre-allocate output\n",
        "n_samples = len(y_indices)\n",
        "X = np.zeros((n_samples, 64), dtype=np.float32)  # 64 bands\n",
        "found_samples = np.zeros(n_samples, dtype=bool)\n",
        "\n",
        "# Process each tile\n",
        "with tqdm(total=len(tile_files), desc=\"Processing tiles\", unit=\" tiles\") as pbar:\n",
        "    for tile_file in tile_files:\n",
        "        # Open tile\n",
        "        with rasterio.open(tile_file) as tile_src:\n",
        "            # Get tile position from filename\n",
        "            # Format: bow_river_embeddings_2020_CORRECTED-RRRRRRRRRR-CCCCCCCCCC.tif\n",
        "            parts = tile_file.stem.split('-')\n",
        "            if len(parts) >= 3:\n",
        "                try:\n",
        "                    tile_row_offset = int(parts[-2])\n",
        "                    tile_col_offset = int(parts[-1])\n",
        "                except ValueError:\n",
        "                    # Fallback: skip this tile if filename parsing fails\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "            else:\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "            \n",
        "            # Get tile dimensions\n",
        "            tile_height, tile_width = tile_src.height, tile_src.width\n",
        "            \n",
        "            # Find which samples fall within this tile\n",
        "            in_tile_y = (y_indices >= tile_row_offset) & (y_indices < tile_row_offset + tile_height)\n",
        "            in_tile_x = (x_indices >= tile_col_offset) & (x_indices < tile_col_offset + tile_width)\n",
        "            in_tile_mask = in_tile_y & in_tile_x\n",
        "            \n",
        "            if in_tile_mask.any():\n",
        "                # Read tile data\n",
        "                tile_data = tile_src.read()  # Shape: (64, height, width)\n",
        "                \n",
        "                # Get local coordinates within this tile\n",
        "                local_y = y_indices[in_tile_mask] - tile_row_offset\n",
        "                local_x = x_indices[in_tile_mask] - tile_col_offset\n",
        "                \n",
        "                # Extract embeddings for these samples\n",
        "                for i, (ly, lx) in enumerate(zip(local_y, local_x)):\n",
        "                    global_idx = np.where(in_tile_mask)[0][i]\n",
        "                    pixel_values = tile_data[:, ly, lx]\n",
        "                    \n",
        "                    # ONLY filter NaN values - keep everything else!\n",
        "                    if not np.isnan(pixel_values).any():\n",
        "                        X[global_idx, :] = pixel_values\n",
        "                        found_samples[global_idx] = True\n",
        "        \n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({\"found\": f\"{found_samples.sum():,}/{n_samples:,}\"})\n",
        "\n",
        "print(f\"\\nâœ“ Extracted {found_samples.sum():,} / {n_samples:,} samples\")\n",
        "\n",
        "if not found_samples.all():\n",
        "    missing = (~found_samples).sum()\n",
        "    print(f\"   âš  Warning: {missing:,} samples not found (may have NaN values)\")\n",
        "    \n",
        "    # Show which classes were affected\n",
        "    print(\"\\n   Missing samples by class:\")\n",
        "    for cls in unique_classes:\n",
        "        cls_mask = (y == cls)\n",
        "        missing_in_class = (~found_samples[cls_mask]).sum()\n",
        "        total_in_class = cls_mask.sum()\n",
        "        if missing_in_class > 0:\n",
        "            print(f\"     Class {cls}: {missing_in_class:,} / {total_in_class:,} missing\")\n",
        "\n",
        "print(\"\\nâœ… Embedding extraction complete!\")"
      ],
      "metadata": {
        "id": "extract_embeddings"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: CALCULATE CLASS WEIGHTS\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CALCULATING CLASS WEIGHTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Only use samples we successfully found\n",
        "y_valid = y[found_samples]\n",
        "\n",
        "# Calculate class weights for balanced training\n",
        "unique_sampled, sampled_counts = np.unique(y_valid, return_counts=True)\n",
        "class_weights = torch.zeros(6)\n",
        "\n",
        "for cls, count in zip(unique_sampled, sampled_counts):\n",
        "    class_weights[cls] = 1.0 / count\n",
        "\n",
        "# Normalize so they sum to 6 (number of classes)\n",
        "class_weights = class_weights / class_weights.sum() * 6\n",
        "\n",
        "print(\"\\nClass weights for training:\")\n",
        "for cls in range(6):\n",
        "    if cls in unique_sampled:\n",
        "        print(f\"   Class {cls}: {class_weights[cls]:.4f}\")\n",
        "    else:\n",
        "        print(f\"   Class {cls}: MISSING\")\n",
        "\n",
        "print(\"\\nâœ… Class weights calculated!\")"
      ],
      "metadata": {
        "id": "class_weights"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: SAVE DATASET\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING DATASET\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nSaving to: {output_file}\")\n",
        "\n",
        "# Save only valid samples (no NaN)\n",
        "np.savez_compressed(\n",
        "    output_file,\n",
        "    X=X[found_samples],\n",
        "    y=y[found_samples],\n",
        "    class_weights=class_weights.numpy(),\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"âœ… DATASET CREATION COMPLETE!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nDataset: {output_file}\")\n",
        "print(f\"Samples: {found_samples.sum():,}\")\n",
        "print(f\"Features: 64\")\n",
        "print(f\"Size: {X[found_samples].nbytes / (1024**3):.2f} GB in memory\")\n",
        "print(f\"\\nFinal class distribution:\")\n",
        "for cls, count in zip(unique_sampled, sampled_counts):\n",
        "    pct = 100 * count / found_samples.sum()\n",
        "    print(f\"  Class {cls}: {count:,} samples ({pct:.2f}%)\")\n",
        "print(f\"\\nUse in PyTorch: nn.CrossEntropyLoss(weight=class_weights)\")"
      ],
      "metadata": {
        "id": "save_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: VERIFY SAVED FILE\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load and verify the saved file\n",
        "data = np.load(output_file)\n",
        "\n",
        "print(f\"\\nLoaded from: {output_file}\")\n",
        "print(f\"\\nArrays in file: {list(data.keys())}\")\n",
        "\n",
        "for key in data.keys():\n",
        "    arr = data[key]\n",
        "    print(f\"\\n{key}:\")\n",
        "    print(f\"  Shape: {arr.shape}\")\n",
        "    print(f\"  Type: {arr.dtype}\")\n",
        "    \n",
        "    if key == 'X':\n",
        "        print(f\"  Has NaN: {np.isnan(arr).any()}\")\n",
        "        print(f\"  Has Inf: {np.isinf(arr).any()}\")\n",
        "        print(f\"  Min: {arr.min():.4f}, Max: {arr.max():.4f}\")\n",
        "    elif key == 'y':\n",
        "        unique, counts = np.unique(arr, return_counts=True)\n",
        "        print(f\"  Classes: {unique}\")\n",
        "\n",
        "data.close()\n",
        "\n",
        "print(\"\\nâœ… Verification complete!\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸŽ‰ READY FOR MODEL TRAINING!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "verify"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
